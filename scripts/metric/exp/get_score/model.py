#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
sample the similar amount of generations as the CHILDES distribution

@author: jliu

"""
import sys
import os
import pandas as pd
import collections
import argparse
import enchant
#from util import lemmatize

#d = enchant.Dict("en_US")

def parseArgs(argv):
    # Run parameters
    parser = argparse.ArgumentParser(description='Concatenate the generated tokens by model')
    
    parser.add_argument('--root_path', type=str, default = '/data/exp/generation',
                        help='root_path to the generated tokens')
    
    parser.add_argument('--out_path', type=str, default = '/data/exp/',
                        help='root_path to the generated tokens')
    
    parser.add_argument('--prompt_type', type=str, default = 'unprompted',
                        help='prompt_type of the generation: prompted or unprompted')
    
    parser.add_argument('--strategy', type=str, default = 'sample_random',
                        help='decoding method to evaluate')
    
    parser.add_argument('--temp', default = 1.5,
                        help='temperature to evaluate')
    
    parser.add_argument('--word_per_sec', type=int, default = 3,
                        help='the estimated number of words per second')
    
    parser.add_argument('--hour', type=int, default = 10,
                        help=' number of hours per day')
    
    parser.add_argument('--sec_frame_path', type=str, default = '/data/Machine_CDI/Lexical-benchmark_data/exp/vocal_month.csv',
                        help='the estmated vocalization seconds per hour by month')
    
    return parser.parse_args(argv)


root_path = '/data/exp/generation'
prompt_type = 'unprompted'
strategy = 'sample_random'

word_per_sec = 3
hour = 10
sec_frame_path = '/data/Machine_CDI/Lexical-benchmark_data/exp/vocal_month.csv'
sec_frame = pd.read_csv(sec_frame_path)

def load_data(root_path, prompt_type,strategy,temp,word_per_sec,hour,sec_frame):
    

    '''
    count all the words in the generated tokens 
    
    input: the root directory containing all the generated files adn train reference
    output: 1.the info frame with all the generarted tokens
            2.the reference frame with an additional column of the month info
            3.vocab size frame with the seq word and lemma frequencies
    '''
    
    month_dict = {'50h':[1],'100h':[1],'200h':[2,3],'400h':[4,8],'800h':[9,18],'1600h':[19,28],'3200h':[29,36]}
    
    generation_all = pd.DataFrame()
    seq_all = []
    # go over the generated files recursively
    for month in os.listdir(root_path): 
            
        for chunk in os.listdir(root_path + '/' + month): 
            
            try:
                for file in os.listdir(root_path + '/' + month + '/' +  chunk+ '/' + prompt_type + '/' + strategy):
                    
                    print(root_path + '/' + month + '/' +  chunk+ '/' + prompt_type + '/' + strategy + '/' + file)
                    '''
                    if not file.split('_')[1] == str(temp):
                        print(file.split('_')[1])
                    # load the file with generated by the target hyperpara 
                    elif file.split('_')[1] == str(temp):
                        
                        print('Loaded: ' + month+ '/' + prompt_type + '/' + strategy + '/' + file)  
                        
                        data = pd.read_csv(root_path + '/' + month + '/' +  chunk+ '/' + prompt_type + '/' + strategy + '/' + file)
                        data['month'] = data['month'].astype(int)
                        # sample data of the corresponding month
                        result = data.loc[(data['month'] >= month_dict[month][0]) & (data['month'] <= month_dict[month][-1])]
                        generation_all = pd.concat([generation_all,result])   
                        
                        n = 0
                        while n < result.shape[0]:
                            generated = result['LSTM_segmented'].tolist()[n].split(' ')
                            seq_all.extend(generated)
                            n += 1
                     '''             
            except:
                    print('Skip: ' + month + '/' +  chunk+ '/' + prompt_type + '/' + strategy + '/' + file)
    
    # get the list of all the words
    seq_lst_temp = list(set(seq_all))
    seq_lst = [element for element in seq_lst_temp if element.strip() != '']   
    cleaned_frame = pd.DataFrame(seq_lst)
    
    for i in list(set(generation_all['month'].tolist())):
        cleaned_frame[i] = [0] * len(seq_lst)
    
    cleaned_frame.set_index(0, inplace=True)
    
    # Fill the DataFrame with zeros for the specified number of rows
    for month in list(set(generation_all['month'].tolist())):
        
        try:
            sec_per_hour = sec_frame[sec_frame['month']==month]['sec_per_hour'].item()
                    
            selected_frame = generation_all[generation_all['month']==month]
            seq = []
            n = 0
            while n < selected_frame.shape[0]:
                generated = selected_frame['LSTM_segmented'].tolist()[n].split(' ')
                seq.extend(generated)
                n += 1
            # get the frequency that corresponds to each month
            filtered_seq = [element for element in seq if element.strip() != '']       
            # get freq lists
            frequencyDict = collections.Counter(filtered_seq)  
            
            for word, freq in frequencyDict.items():
                    cleaned_frame.loc[word,month] = freq/len(filtered_seq)* 30 * word_per_sec * hour * sec_per_hour
                    n += 1
        except:
            print(month)
    
    # get cumulative frequency
    seq_frame = cleaned_frame.cumsum(axis=1)
    return seq_frame
    '''
    # get the lemam frame and then match the results
    lemma_dict = lemmatize(seq_lst)
    
    lemma_frame = pd.DataFrame()
    for lemma, words in lemma_dict.items():
        # Merge columns in the list by adding their values
        # Sum all rows to get one row with summed values
        df = seq_frame.loc[words]
        summed_row = df.sum(axis=0)
        # Convert the resulting Series back to a DataFrame with one row
        summed_df = pd.DataFrame([summed_row], columns=summed_row.index)
        lemma_frame = pd.concat([lemma_frame,summed_df])
    
    lemma_frame.index = list(lemma_dict.keys())
    
    return seq_frame, lemma_frame
    '''

root_path = '/data/exp/prompted_1.5'
temp = 1.5

def load_data(root_path, prompt_type,strategy,temp,word_per_sec,hour,sec_frame):
    

    '''
    count all the words in the generated tokens 
    
    input: the root directory containing all the generated files adn train reference
    output: 1.the info frame with all the generarted tokens
            2.the reference frame with an additional column of the month info
            3.vocab size frame with the seq word and lemma frequencies
    '''
    
    #month_dict = {'50':[1],'100':[1],'200':[2,3],'400':[4,8],'800':[9,18],'1600':[19,28],'3200':[29,36]}
    month_dict = {'400':[4,8],'800':[9,18],'3200':[29,36]}
    generation_all = pd.DataFrame()
    seq_all = []
    # go over the generated files recursively
    
    for file in os.listdir(root_path):
                    
                        month = file.split('.')[0]
                        
                        data = pd.read_csv(root_path + '/' + file)
                        data['month'] = data['month'].astype(int)
                        # sample data of the corresponding month
                        result = data.loc[(data['month'] >= month_dict[month][0]) & (data['month'] <= month_dict[month][-1])]
                        generation_all = pd.concat([generation_all,result])   
                        
                        n = 0
                        while n < result.shape[0]:
                            generated = result['LSTM_segmented'].tolist()[n].split(' ')
                            seq_all.extend(generated)
                            n += 1
                                 
                        print(file)
    # get the list of all the words
    seq_lst_temp = list(set(seq_all))
    seq_lst = [element for element in seq_lst_temp if element.strip() != '']   
    cleaned_frame = pd.DataFrame(seq_lst)
    
    for i in list(set(generation_all['month'].tolist())):
        cleaned_frame[i] = [0] * len(seq_lst)
    
    cleaned_frame.set_index(0, inplace=True)
    
    # Fill the DataFrame with zeros for the specified number of rows
    for month in list(set(generation_all['month'].tolist())):
        
        try:
            sec_per_hour = sec_frame[sec_frame['month']==month]['sec_per_hour'].item()
                    
            selected_frame = generation_all[generation_all['month']==month]
            seq = []
            n = 0
            while n < selected_frame.shape[0]:
                generated = selected_frame['LSTM_segmented'].tolist()[n].split(' ')
                seq.extend(generated)
                n += 1
            # get the frequency that corresponds to each month
            filtered_seq = [element for element in seq if element.strip() != '']       
            # get freq lists
            frequencyDict = collections.Counter(filtered_seq)  
            
            for word, freq in frequencyDict.items():
                    cleaned_frame.loc[word,month] = freq/len(filtered_seq)* 30 * word_per_sec * hour * sec_per_hour
                    n += 1
        except:
            print(month)
    
    # get cumulative frequency
    seq_frame = cleaned_frame.cumsum(axis=1)
    
    # get the lemam frame and then match the results
    lemma_dict = lemmatize(seq_lst)
    
    lemma_frame = pd.DataFrame()
    for lemma, words in lemma_dict.items():
        # Merge columns in the list by adding their values
        # Sum all rows to get one row with summed values
        df = seq_frame.loc[words]
        summed_row = df.sum(axis=0)
        # Convert the resulting Series back to a DataFrame with one row
        summed_df = pd.DataFrame([summed_row], columns=summed_row.index)
        lemma_frame = pd.concat([lemma_frame,summed_df])
    
    lemma_frame.index = list(lemma_dict.keys())
    
    return seq_frame, lemma_frame
    

seq_frame.to_csv('prompted_1.5_new.csv')



def main(argv):
    
    # Args parser
    args = parseArgs(argv)
    root_path = args.root_path
    strategy = args.strategy
    word_per_sec = args.word_per_sec
    hour = args.hour
    out_path = args.out_path
    prompt_type = args.prompt_type
    temp = args.temp
    sec_frame = pd.read_csv(args.sec_frame_path)
    seq_frame = load_data(root_path, prompt_type,strategy,temp,word_per_sec,hour,sec_frame)
                
    seq_frame.to_csv(out_path + prompt_type + '_' + str(temp) + '_seq.csv')  
    #lemma_frame.to_csv(out_path + 'prompted_lemma.csv')  
    

if __name__ == "__main__":
    args = sys.argv[1:]
    main(args)
    
    
  

