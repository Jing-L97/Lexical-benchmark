#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
sample the similar amount of generations as the CHILDES distribution

@author: jliu

"""
import os
import pandas as pd
import collections
import enchant

d = enchant.Dict("en_US")


root_path = '/data/exp/generation'
prompt_type = 'unprompted'
strategy = 'sample_random'
temp = 1.5
word_per_sec = 3
hour = 10
sec_frame_path = '/data/Machine_CDI/Lexical-benchmark_data/exp/vocal_month.csv'
sec_frame = pd.read_csv(sec_frame_path)

def load_data(root_path, prompt_type,strategy,temp,word_per_sec,hour,sec_frame):
    

    '''
    count all the words in the generated tokens 
    
    input: the root directory containing all the generated files adn train reference
    output: 1.the info frame with all the generarted tokens
            2.the reference frame with an additional column of the month info
            3.vocab size frame with the seq word and lemma frequencies
    '''
    
    #month_dict = {'50h':[1],'100h':[1],'200h':[2,3],'400h':[4,8],'800h':[9,18],'1600h':[19,28],'3200h':[29,36]}
    generation_all = pd.DataFrame()
    seq_all = []
    # go over the generated files recursively
    for month in os.listdir(root_path): 
        for chunk in os.listdir(root_path + '/' + month): 
            
            try:
                for file in os.listdir(root_path + '/' + month + '/' +  chunk+ '/' + prompt_type + '/' + strategy):
                
                    # load the file with generated by the target hyperpara 
                    if file.split('_')[1] == str(temp):
                        
                        print('Loaded: ' + month+ '/' + prompt_type + '/' + strategy + '/' + file)  
                        data = pd.read_csv(root_path + '/' + month + '/' +  chunk+ '/' + prompt_type + '/' + strategy + '/' + file)
                        
                        
                        data['month'] = data['month'].astype(int)
                        # sample data of the corresponding month
                        #result = data.loc[(data['month'] >= month_dict[month][0]) & (data['month'] <= month_dict[month][-1])]
                        generation_all = pd.concat([generation_all,data])   
                        
                        n = 0
                        while n < result.shape[0]:
                            generated = result['LSTM_segmented'].tolist()[n].split(' ')
                            seq_all.extend(generated)
                            n += 1
                        
                              
            except:
                    print('Skip: ' + month + '/' +  chunk+ '/' + prompt_type + '/' + strategy + '/' + file)
    
    # get the list of all the words
    seq_lst_temp = list(set(seq_all))
    seq_lst = [element for element in seq_lst_temp if element.strip() != '']   
    cleaned_frame = pd.DataFrame(seq_lst)
    
    for i in list(set(generation_all['month'].tolist())):
        cleaned_frame[i] = [0] * len(seq_lst)
    
    cleaned_frame.set_index(0, inplace=True)
    
    # Fill the DataFrame with zeros for the specified number of rows
    for month in list(set(generation_all['month'].tolist())):
        
        try:
            sec_per_hour = sec_frame[sec_frame['month']==month]['sec_per_hour'].item()
                    
            selected_frame = generation_all[generation_all['month']==month]
            seq = []
            n = 0
            while n < selected_frame.shape[0]:
                generated = selected_frame['LSTM_segmented'].tolist()[n].split(' ')
                seq.extend(generated)
                n += 1
            # get the frequency that corresponds to each month
            filtered_seq = [element for element in seq if element.strip() != '']       
            # get freq lists
            frequencyDict = collections.Counter(filtered_seq)  
            
            for word, freq in frequencyDict.items():
                    cleaned_frame.loc[word,month] = freq/len(filtered_seq)* 30 * word_per_sec * hour * sec_per_hour
                    n += 1
        except:
            print(month)
    
    # get cumulative frequency
    seq_frame = cleaned_frame.cumsum(axis=1)
    return seq_frame
    

root_path = '/data/exp/unprompted_1.5'
temp = 1.5
train_path = '/data/Machine_CDI/Lexical-benchmark_data/exp/Audiobook/freq_by_train'


def load_data(root_path, prompt_type,strategy,temp,word_per_sec,hour,sec_frame,
              train_path,month_corre,analysis_type):
    
    '''
    concatenate the scripts of different chunks
    '''
    generation_all = pd.DataFrame()
    prop_frame = pd.DataFrame()
    if analysis_type == 'type':
        seq_all = set()
    else:
        seq_all = []
    if not month_corre: 
        train = pd.read_csv(train_path + '/3200.csv')
    # go over the generated files recursively
    for file in os.listdir(root_path):
        month = file.split('.')[0]
        data = pd.read_csv(root_path + '/' + file)
        if month_corre: 
            train = pd.read_csv(train_path + '/' + file)
        # take the largest set as thre train set 
        
        data['month'] = data['month'].astype(int)
        # sample data of the corresponding month
        generation_all = pd.concat([generation_all,data])   
                       
        n = 0
        
        if analysis_type == 'type':
            seq_month = set()
            oov_tokens = set()
            oov_words = set()
            
            while n < data.shape[0]:
                generated = data['LSTM_segmented'].tolist()[n].split(' ')
                generated = [x for x in generated if x != ""]
                for word in generated:
                    if not word.isspace():
                        seq_month.add(word)
                        seq_all.add(word)
                        if word not in train['Word'].tolist():
                            oov_tokens.add(word)
                            if d.check(word) == True:
                                oov_words.add(word)
                                
                n += 1
        else:
            seq_month = []
            oov_tokens = []
            oov_words = []
            
            while n < data.shape[0]:
                generated = data['LSTM_segmented'].tolist()[n].split(' ')
                generated = [x for x in generated if x != ""]
                for word in generated:
                    if not word.isspace():
                        seq_month.append(word)
                        seq_all.append(word)
                        if word not in train['Word'].tolist():
                            oov_tokens.append(word)
                            if d.check(word) == True:
                                oov_words.append(word)
                                
                n += 1
            
        # get the prop of the oov words
        prop_frame_single = pd.DataFrame([str(file)[:-4],len(oov_words)/len(seq_month),
                                          1 - len(oov_words)/len(seq_month),len(oov_tokens)/len(seq_month)])
        prop_frame = pd.concat([prop_frame,prop_frame_single.T])
        seq_frame_month = pd.DataFrame(seq_month)
        seq_frame_month.to_csv(outpath + 'seq_' + file)
        oov_token_frame = pd.DataFrame(oov_tokens)
        oov_token_frame.to_csv(outpath + 'token_' + file)
        oov_word_frame = pd.DataFrame(oov_words)
        oov_word_frame.to_csv(outpath + 'word_' + file)
        print('Finished counting ' + str(file))
    
    prop_frame.columns = ['model','oov_word_prop','oov_nonword_prop','oov_token_prop']
    prop_frame.to_csv(outpath + 'prop.csv')
    # get the list of all the words
    seq_lst_temp = list(seq_all)
    seq_lst = [element for element in seq_lst_temp if element.strip() != '']   
    cleaned_frame = pd.DataFrame(seq_lst)
    
    for i in list(set(generation_all['month'].tolist())):
        cleaned_frame[i] = [0] * len(seq_lst)
    
    cleaned_frame.set_index(0, inplace=True)
    
    # Fill the DataFrame with zeros for the specified number of rows
    for month in list(set(generation_all['month'].tolist())):
        try:
            sec_per_hour = sec_frame[sec_frame['month']==month]['sec_per_hour'].item()
            selected_frame = generation_all[generation_all['month']==month]
            seq = []
            n = 0
            while n < selected_frame.shape[0]:
                generated = selected_frame['LSTM_segmented'].tolist()[n].split(' ')
                seq.extend(generated)
                n += 1
            # get the frequency that corresponds to each month
            filtered_seq = [element for element in seq if element.strip() != '']       
            # get freq lists
            frequencyDict = collections.Counter(filtered_seq)  
            for word, freq in frequencyDict.items():
                cleaned_frame.loc[word,month] = freq
                    
        except:
            print(month)
    # get cumulative frequency
    cleaned_frame.to_csv(outpath + 'count.csv')
    seq_frame = cleaned_frame.cumsum(axis=1)
    seq_frame.to_csv(outpath + 'cum_count.csv')
    return seq_frame




month_corre = False
prompt_type_lst = ['unprompted','prompted']
temp_lst = [0.3,1.5]
analysis_type = 'number'
outpath_root = '/data/exp/oov_words/' + analysis_type + '/'

for prompt_type in prompt_type_lst:
    for temp in temp_lst:
        
        root_path = '/data/exp/' + prompt_type + '_' + str(temp)
    
        if not month_corre:
            folder = 'largest_set'
            
        if month_corre:
            folder = 'month_correspondent'
            
        outpath = outpath_root + folder + '/oov_audiobook/' + prompt_type + '/' + str(temp) + '/' 
        
        if not os.path.exists(outpath):
             os.makedirs(outpath)
             
        load_data(root_path, prompt_type,strategy,temp,word_per_sec,hour,sec_frame
                  ,train_path,month_corre,analysis_type)
        
        
        
        
        
        
        
        
        
        
        
        
        
        