import typing as t
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy import optimize as scipy_opt  # type:ignore[import-untyped]

from lm_benchmark import nlp_tools, settings

RANDOM_SEED = np.random.default_rng()
# Revised color dictionary
color_dict = {
    "human_CDI": "#d62728",  # Blue
    "CHILDES": "orange",  # Orange
    "adult": "orange",
    "child": "orange",  # Green
    "Accumulator": "#aec7e8",
    "train": "#17becf",  # Red
    "ind": "#17becf",  # Purple
    "ood": "orange",  # Brown
    "crp": "#e377c2",  # Pink
    "0.3": "#7f7f7f",  # Grey
    "0.6": "#8c564b",  # Yellow
    "0.8": "#2ca02c",  # Cyan
    "1.0": "#9467bd",  # Light Blue
    "1.5": "#1f77b4",  # Light Orange
    "prompted_LSTM": "#98df8a",  # Light Green
    "unprompted_LSTM": "#ff9896",  # Light Red
    "prompted_Transformer": "#bcbd22",  # Light Purple
    "unprompted_Transformer": "#c49c94",  # Light Brown
    "child production": "orange",
}

################################################################################################
# function to load csv data#
#################################################################################################


def load_csv(file_path: Path, left_header: str, right_header: str) -> pd.DataFrame:
    """Read the CSV file and load as DataFrame."""
    df = pd.read_csv(file_path)
    df = df.set_index("word")
    # Get the index of the start column
    df = df.loc[:, f"{left_header}" : f"{right_header}"]  # type: ignore[misc] # Ingores slice type
    # Convert column headers to integers
    df.columns = df.columns.astype(int)
    # df = df.sort_values(by='count').reset_index(drop=True)
    return df


def apply_threshold(df: pd.DataFrame, threshold: float) -> pd.DataFrame:
    """Apply a threshold to a DataFrame."""
    return df.map(lambda x: 1 if x > threshold else 0)


def to_tc_df(df: pd.DataFrame, month: int, threshold: float) -> pd.DataFrame:
    """Convert the monthly df into the TC df."""
    is_word = nlp_tools.make_en_word_checker()

    # select the corresponding month
    df_frame = df[[month]]
    # remove words not produced/generated by this month
    df_frame = df_frame[df_frame[month] != 0]
    # apply the threshold on the generated words
    score_frame = apply_threshold(df_frame, threshold)
    score_frame.columns = pd.Index(["count"])
    # convert the result into freq_m
    score_frame["freq_m"] = score_frame["count"] / score_frame["count"].sum() * 1000000
    score_frame = score_frame.reset_index()
    score_frame["correct"] = score_frame["word"].apply(is_word)
    return score_frame


def dict2df(score_dict: dict, header: str) -> pd.DataFrame:
    """Convert the dict into df for plotting rates in E1.3 figs."""
    df = pd.DataFrame(list(score_dict.items()), columns=["month", header])
    df = df.T
    df.columns = pd.Index(df.iloc[0])
    return df.drop(df.index[0])


def month2model(model_dict: dict, value: float) -> str | None:
    """Convert the month into the corresponding trained model."""
    for key, value_range in model_dict.items():
        # Check if the value falls within the range
        if (
            len(value_range) == 1
            and value == value_range[0]
            or len(value_range) == 2
            and value_range[0] <= value <= value_range[1]
        ):
            return key
    return None


def merge_score(df: pd.DataFrame) -> pd.DataFrame:
    """Merge rows of DataFrame."""
    df.columns = df.columns.astype(int)
    average_values = df.mean()
    return pd.DataFrame(average_values).T


def get_equal_quantity(df: pd.DataFrame, col_header: str, n_bins: int) -> pd.DataFrame:
    """Sort the DataFrame based on the selected column."""
    df.dropna()
    df[col_header] = df[col_header].astype(int)
    df = df.sort_values(by=[col_header]).reset_index(drop=True)
    data = df[col_header]
    size = len(data)
    if n_bins <= size:
        raise ValueError("too many bins compared to data size")

    bin_indices = np.linspace(1, len(data), n_bins + 1) - 1  # indices to edges in sorted data
    data_sorted = np.sort(data)
    bins = [
        data_sorted[0],  # left edge inclusive
        *[(data_sorted[int(b)] + data_sorted[int(b + 1)]) / 2 for b in bin_indices[1:-1]],
        data_sorted[-1],  # this is because the extreme right edge is inclusive in plt.hits
    ]
    # computing bin membership for the original data; append bin membership to stat
    bin_membership = np.zeros(size, dtype=int)
    for i in range(len(bins) - 1):
        bin_membership[(data_sorted >= bins[i]) & (data_sorted < bins[i + 1])] = i
    df["group"] = bin_membership
    return df


#################################################################################################
# Summary statistics for Token Counts
#################################################################################################


def load_files(
    freq_root: Path,
) -> tuple[list[nlp_tools.TokenCount], nlp_tools.TokenCount | None, list[nlp_tools.TokenCount]]:
    """Load TC objects recursively."""
    all_count = []
    # load all the generated sets
    for file in sorted(freq_root.iterdir(), key=lambda x: x.name):
        if file.suffix == ".csv":
            word_count = nlp_tools.TokenCount()
            word_count.df = pd.read_csv(file)
            word_count.name = file.name  # WTF: file[:-4].split("_")[-1]
            all_count.append(word_count)

    # divide into different types of sets
    gen_count = []
    unwanted_lst = ["train", "child", "adult"]
    ref_count = None
    for tc in all_count:
        if tc.name not in unwanted_lst:
            gen_count.append(tc)
        if tc.name == "train":
            ref_count = tc

    return all_count, ref_count, gen_count


def custom_format(num) -> str:  # noqa: ANN001
    """Custom numerical format."""
    # f Check if the number is effectively an integer
    if isinstance(num, int):
        return f"{int(num)}"

    # Check if the number is between 1000 and 0.001
    if 0.001 < abs(num) < 1000:
        return f"{num:.3g}"

    # For all other cases, print in scientific notation
    return f"{num:.3e}"


### defining summary statistics over tc and comparison functions
def tc_summary(data: nlp_tools.TokenCount | list[nlp_tools.TokenCount]) -> pd.DataFrame:
    """Returns basic statistics about a list of token_counts.

    As defined in the TokenCount Class, the output is a dataframe with
    statistics as columns and corpora as lines.
    """
    if isinstance(data, nlp_tools.TokenCount):
        data = [data]

    # Compute stat summary
    listofdic = [tc.stats() for tc in data]
    tc_stats = pd.DataFrame(listofdic)

    tc_stats = tc_stats.sort_values(by="name")
    return tc_stats.set_index("name")


# TODO(@Jing): Unfinished function?
def prop_zero(ref_count: nlp_tools.TokenCount, gen_count: nlp_tools.TokenCount) -> None:
    """Computes a plot of differences."""
    refdf = ref_count.df
    gendf = gen_count.df

    missing_count = ref_count.difference(gen_count)


def tc_compare(
    ref_count: nlp_tools.TokenCount,
    gen_count_list: list[nlp_tools.TokenCount],
) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Compare two token counts.

    The first one is the reference, the second one the generated (or test)
    from the reference, one can compute the missing words (words in ref not in gen)
    and the oovs (words in gen not in ref)
    the function retuns two dataframes, one with stats for the missing and one with stats from the oovs
    """
    listofdic = []
    for gen_count in gen_count_list:
        missing_count = ref_count.difference(gen_count)
        m = missing_count.stats()
        m["prop_missing"] = missing_count.nb_of_types() / ref_count.nb_of_types()
        m["name"] = gen_count.name
        listofdic.append(m)
    missing_stats = pd.DataFrame(listofdic)
    missing_stats = missing_stats.set_index("name")

    listofdic = []
    for gen_count in gen_count_list:
        oov_count = gen_count.difference(ref_count)
        m = oov_count.stats()
        m["prop_oovs"] = oov_count.nb_of_types() / gen_count.nb_of_types()
        m["name"] = gen_count.name
        listofdic.append(m)
    oov_stats = pd.DataFrame(listofdic)
    oov_stats = oov_stats.set_index("name")

    listofdic = []
    for gen_count in gen_count_list:
        try:
            oov_count = gen_count.difference(ref_count)
            nword_count = oov_count.non_word()
            m = nword_count.stats()
            m["prop_nwords"] = nword_count.nb_of_types() / gen_count.nb_of_types()
            m["name"] = gen_count.name
            listofdic.append(m)
        except ValueError:
            pass
    nword_stats = pd.DataFrame(listofdic)
    nword_stats = nword_stats.set_index("name")

    return missing_stats, oov_stats, nword_stats


def calculate_miss_scores(group: pd.DataFrame) -> pd.Series:
    """Function used in a groupby bin.

    Computes various scores for the missing items (from the point of view of ref_count).
    """
    dfreq_score = (group["gen_count"] < group["ref_count"]).mean() * -1 + (
        group["gen_count"] > group["ref_count"]
    ).mean()
    pmiss = (group["gen_count"] == 0).mean()
    nb = group.shape[0]
    medcount = (group["ref_count"]).median()

    # Return result
    return pd.Series([medcount, dfreq_score, pmiss, nb], index=["medcount", "dfreq_score", "pmiss", "nb"])


def calculate_oov_scores(group: pd.DataFrame) -> pd.Series:
    """Function used in a groupby bin; computes various scores for the oov items."""
    poov = (group["ref_count"] == 0).mean()
    nb = group.shape[0]
    medcount = (group["gen_count"]).median()
    return pd.Series([medcount, poov, nb], index=["medcount", "poov", "nb"])


def calculate_word_scores(group: pd.DataFrame) -> pd.Series:
    """Function used in a groupby bin; computes various scores for the oov items."""
    pnonword = ((group["correct"] != False) & (group["ref_count"] == 0)).mean()
    nb = group.shape[0]
    medcount = (group["gen_count"]).median()
    return pd.Series([medcount, pnonword, nb], index=["medcount", "pnword", "nb"])


def build_bins(df, count_header: str, groupbin: int) -> pd.DataFrame:
    """Bin data into groups."""
    nblines = len(df)
    num_bins = groupbin  # int(nblines/groupbin)

    df["rnd"] = df[count_header] + RANDOM_SEED.normal(loc=0, scale=0.1, size=nblines)
    df["ranks"] = df["rnd"].rank().astype(int)
    df["bin"] = pd.qcut(df["ranks"], q=num_bins, labels=[f"Bin_{i + 1}" for i in range(num_bins)])
    return df


def tc_compute_miss_oov_rates(
    ref_count: nlp_tools.TokenCount,
    gen_count: nlp_tools.TokenCount,
    groupbin: int = 20,
) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Computes miss rate and oov rate curves.

    From two token counts, one reference and one generated or test.
    Probability as a function of token count, grouped by bins of groupbin size.
    """
    # set the word column as index
    ref_count_frame = ref_count.df[["word", "count", "correct"]]
    gen_count_frame = gen_count.df[["word", "count", "correct"]]
    ref_count_frame = ref_count_frame.set_index("word")
    gen_count_frame = gen_count_frame.set_index("word")

    # remove the index with na
    ref_count_frame = ref_count_frame[~ref_count_frame.index.isna()]
    gen_count_frame = gen_count_frame[~gen_count_frame.index.isna()]

    # make a joint dataframe of counts for ref and gen (missing words are indicated by nans)
    newdf = pd.concat([ref_count_frame, gen_count_frame], axis=1, join="outer")

    # renaming the columns
    newdf.columns = pd.Index(["ref_count", "ref_correct", "gen_count", "correct"])

    # merge the spelling check columns
    newdf["correct"] = newdf["correct"].fillna(newdf["ref_correct"])
    newdf = newdf.drop("ref_correct", axis=1)  # remove the duplicated spelling check column
    newdf = newdf.fillna(0)

    # this is a df where the oovs have been removed (to compute missing rates)
    missingdf = newdf.copy(deep=True)[newdf["ref_count"] != 0]

    # building bins of 200 tokens for the missing rate curve
    missingdf = build_bins(missingdf, "ref_count", groupbin)
    mscores = missingdf.groupby("bin").apply(calculate_miss_scores).reset_index()

    # this is a df where the missed words have been removed (to compute oov rates)
    oovdf = newdf.copy(deep=True)[newdf["gen_count"] != 0]
    oovdf = build_bins(oovdf, "gen_count", groupbin)
    oscores = oovdf.groupby("bin").apply(calculate_oov_scores).reset_index()
    nscores = oovdf.groupby("bin").apply(calculate_word_scores).reset_index()

    return mscores, oscores, nscores


#################################################################################################
# E1 plotting functions
#################################################################################################
def plot_score(
    df: pd.DataFrame,
    label: str,
    xlim: tuple[int, int] = (0, 36),
    ylim: tuple[int, int] = (0, 1),
    xlabel: str = "(Pseudo) month",
    ylabel: str = "Proportion of acquired words",
    *,
    color: bool = False,
    error_bar: bool = False,
    linewidth: int = 2,
) -> None:
    """Plot the thresholded counts with color range for variability."""
    # Convert column headers to integers
    df.columns = df.columns.astype(int)
    # Calculate average values across rows for each column
    average_values = df.mean()
    # Calculate standard deviation across rows for each column
    std_dev_values = df.std()
    # Plot the curve with color range
    plt.xlabel(xlabel, fontsize=14)  # Label for the x-axis
    plt.ylabel(ylabel, fontsize=14)  # Label for the y-axis
    plt.xlim(xlim)
    plt.ylim(ylim)

    if not color:
        # Fill the area between the average values +/- standard deviation
        plt.plot(
            average_values.index,
            average_values.to_numpy(),
            label=label,
            linewidth=linewidth,
        )
        if error_bar:
            plt.fill_between(
                average_values.index,
                average_values.to_numpy() - std_dev_values.to_numpy(),
                average_values.to_numpy() + std_dev_values.to_numpy(),
                alpha=0.3,
            )
    else:
        plt.plot(average_values.index, average_values.to_numpy(), label=label, color=color, linewidth=linewidth)
        if error_bar:
            plt.fill_between(
                average_values.index,
                average_values.to_numpy() - std_dev_values.to_numpy(),
                average_values.to_numpy() + std_dev_values.to_numpy(),
                color=color,
                alpha=0.3,
            )

    plt.legend()  # Show legend


def fit_sigmoid(score_df: pd.DataFrame, target_y, label: str) -> pd.DataFrame:
    """Fit sigmoid curve of extrapolated exp vocab."""

    def sigmoid(x, a, b) -> np.ndarray:  # noqa: ANN001
        return 1 / (1 + np.exp(-(a * x + b)))

    x_data = score_df.columns.to_numpy()
    y_data = score_df.iloc[0].to_numpy()

    # Fit the sigmoid function to the scatter plot data
    popt, pcov = scipy_opt.curve_fit(sigmoid, x_data, y_data, maxfev=100000, method="trf")
    # Generate x values for the fitted curve
    x_fit = np.linspace(0, max(x_data), 40)
    # Use the optimized parameters to generate y values for the fitted curve
    y_fit = sigmoid(x_fit, *popt)
    # first find the target x in the given scatter plots
    if max(y_data) < target_y:
        # Use the optimized parameters to generate y values for the fitted curve
        y_fit = sigmoid(x_fit, *popt)
        while y_fit[-1] < target_y:
            x_fit = np.append(x_fit, x_fit[-1] + 1)
            y_fit = np.append(y_fit, sigmoid(x_fit[-1], *popt))
            # Break the loop if the condition is met
            if y_fit[-1] >= target_y:
                break

    # Generate x values for the fitted curve
    # Find the index of the target y-value
    target_y_index = np.argmin(np.abs(y_fit - target_y))
    # Retrieve the corresponding x-value
    target_x = x_fit[target_y_index]

    plt.scatter(x_data, y_data)
    # plot until it has reached the target x
    plt.plot(x_fit, y_fit, linewidth=3.5, label=label + f": {target_x:.2f}")
    plt.legend()
    plt.ylim(0, 1)
    # Marking the point where y reaches the target value
    plt.axvline(x=int(target_x), linestyle="dotted")
    header_lst = ["Group", "Month", "Slope", "Weighted_offset"]
    # return the optimized parameters of the sigmoid function
    para_frame = pd.DataFrame([label, target_x, popt[0], popt[1]]).T
    para_frame.columns = pd.Index(header_lst)
    return para_frame


def plot_group(
    cdi_path: Path,
    score_dir: Path,
    month_range: list,
    n_bins: int,
    label: int,
    threshold: int,
) -> pd.DataFrame:
    """Plot group extrapolations."""
    # load CDI files
    frame = pd.read_csv(cdi_path)
    frame = get_equal_quantity(frame, "count", n_bins)
    frame_grouped = frame.groupby("group")

    # load score files
    all_score = load_csv(score_dir, str(month_range[0]), str(month_range[1]))
    for group, frame_group in frame_grouped:
        # select the corresponding words
        df = all_score[all_score.index.isin(frame_group["word"])]
        score = apply_threshold(df, threshold)
        merged = merge_score(score)
        # merge results from different models
        plot_score(merged, label=f"{group}")
        plt.title(f"Vocabulary growth curve in {str(label)[1:-1]}", fontsize=16, fontweight="bold")
    return frame


def plot_sigmoid(
    cdi_path: Path,
    word_freq_path: Path,
    score_dir: Path,
    month_range: list,
    threshold: int,
    n_bins: int,
    target_y: int,
    genset_label: str,
) -> pd.DataFrame:
    # TODO(@Jing) : lang variable not defined ???
    # load files
    word_freq = pd.read_csv(word_freq_path)
    if genset_label == "human_CDI":
        all_score = pd.read_csv(cdi_path)
        all_score = all_score.set_index("word")
        all_score = all_score.loc[:, str(settings.AGE_DICT[lang][0]) : str(settings.AGE_DICT[lang][1])]  # type: ignore[misc] # ignore slice type
    else:
        all_score = load_csv(score_dir, str(month_range[0]), str(month_range[1]))
    frame = pd.read_csv(cdi_path)
    frame = get_equal_quantity(frame, "count", n_bins)
    frame_grouped = frame.groupby("group")

    para_all = pd.DataFrame()
    for _, frame_group in frame_grouped:
        # select the corresponding words
        score = all_score[all_score.index.isin(frame_group["word"])]
        # get log median freq of each bands
        m_freq = np.log10(word_freq[word_freq["word"].isin(frame_group["word"])]["freq_m"].median())
        # apply threhsold if not reported by human
        if genset_label != "human_CDI":
            score = apply_threshold(score, threshold)
        merged = merge_score(score)
        para = fit_sigmoid(merged, target_y, f"{m_freq:.2f}")
        para_all = pd.concat([para_all, para])
        plt.title(f"Vocab growth curve by ({genset_label})")
    plt.show()
    para_all["gen_set"] = genset_label
    return para_all


def fit_log(x_data: np.ndarray, y_data: np.ndarray, label: str) -> None:
    """Fit a logarithmic curve to the data and plot it."""

    def log_curve(x, a, b) -> np.ndarray:  # noqa: ANN001
        return a * np.log10(x) + b

    # Fit the logarithmic function to the scatter plot data
    popt, _ = scipy_opt.curve_fit(log_curve, x_data, y_data, maxfev=100000, method="trf")
    # Generate x values for the fitted curve
    x_fit = np.linspace(min(x_data), max(x_data), 40)
    # Use the optimized parameters to generate y values for the fitted curve
    y_fit = log_curve(x_fit, *popt)
    # Plot the original scatter plot data
    plt.scatter(x_data, y_data)
    # Plot the fitted curve
    plt.plot(x_fit, y_fit, linewidth=3.5, label=label)
    plt.xlabel("Median freq", fontsize=15)
    plt.ylabel("Estimated months", fontsize=15)
    plt.tick_params(axis="both", labelsize=10)
    plt.legend()


def plot_log(para_frame: pd.DataFrame) -> None:
    """Freq sensitivity in different bands."""
    para_frame_grouped = para_frame.groupby(["gen_set"])

    # Iterate over each group
    for group, para_frame_group in para_frame_grouped:
        # Convert 'Type' column from log scale to linear scale
        median_lst = [10**x for x in para_frame_group["Group"].astype(float)]

        # Ensure lengths match
        if len(median_lst) != len(para_frame_group["Month"]):
            print(f"Length mismatch in group {group}: {len(median_lst)} vs {len(para_frame_group['Month'])}")
            continue  # Skip this group if there's a mismatch
        fit_log(np.array(median_lst), para_frame_group["Month"].to_numpy(), str(group)[1:-2])
        # Set plot title and legend
        plt.title("Frequency sensitivity", fontsize=15, fontweight="bold")
        legend_loc = "upper right"
        plt.legend(loc=legend_loc)
        # Set Y-axis limits
        plt.ylim(-10, 190)


def plot_lines(para_frame: pd.DataFrame, prompt_type: str, model: str) -> None:
    """Frequency sensitivity in different bands."""
    para_frame_grouped = para_frame.groupby(["gen_set"])

    # Iterate over each group
    for group, para_frame_group in para_frame_grouped:
        print(group)
        # Convert 'Type' column from log scale to linear scale
        median_lst = para_frame_group["Group"].astype(float)
        plt.scatter(median_lst, para_frame_group["Month"], color=color_dict[str(group[0])])
        plt.plot(
            median_lst, para_frame_group["Month"], label=str(group[0]), linewidth=3, color=color_dict[str(group[0])]
        )
        # Set plot title and legend
        plt.title("Frequency sensitivity", fontsize=15, fontweight="bold")
        legend_loc = "upper right"
        plt.legend(loc=legend_loc)
        # Set Y-axis limits
        plt.ylim(-10, 190)

    plt.xlabel("Median freq", fontsize=15)
    plt.ylabel("Estimated months", fontsize=15)
    plt.title(f"{prompt_type} generations by {model}", fontsize=13, fontweight="bold")


#################################################################################################
# E2 plotting functions
#################################################################################################


def line_plot(
    data: dict[str, pd.DataFrame], fig_path: Path | None = None, *, xlog: bool = True, ylim: tuple[int, int] = (0, 1)
) -> None:
    """Generic line plotting function.

    Sata is a dictionnary associating a name with a dataframe with two columns
    (the first one for x, the second one for y)
    """
    # plt.figure(figsize=(10, 5))  # Set the size of the plot

    for key in data:
        df = data[key]
        xname = df.columns[0]
        yname = df.columns[1]
        aggregated_df = df.groupby(xname)[yname].mean().reset_index()
        plt.plot(aggregated_df[xname], aggregated_df[yname], label=key)
    # plt.title('prob of missing as a function of Token Count for the accumulator model')  # Title of the plot
    plt.xlabel(xname)  # Label for the x-axis
    plt.ylabel(yname)  # Label for the y-axis
    plt.ylim(ylim)
    plt.axhline(y=0, color="red", linestyle="--", label="y = 0")
    if xlog:
        plt.xscale("log")
    plt.grid(True)  # Show grid lines
    plt.legend()  # Show legend
    if fig_path:
        plt.savefig(fig_path, dpi=300, bbox_inches="tight")
    plt.show()  # Display the plot


def bar_plot(
    values: list | np.ndarray,
    names: list[str],
    lower_bounds: list | np.ndarray | None = None,
    upper_bounds: list | np.ndarray | None = None,
    colors: list[str] | None = None,
    ytitle: str | None = None,
    title: str | None = None,
    *,
    showval: bool = True,
) -> None:
    """Creates a bar plot with optional confidence intervals.

    Parameters
    ----------
    values:
        list or array of mean values (center of error bars)
    names:
        list of names for each bar
    lower_bounds:
        list or array of lower bounds of the confidence intervals
    upper_bounds:
        list or array of upper bounds of the confidence intervals
    colors:
        ...
    ytitle:
        ...
    title:
        ...
    showval:
        ...

    Note
    ----
    The lengths of all inputs must match.

    """
    # Calculate the error margins from the values
    if (lower_bounds is not None) and (upper_bounds is not None):
        error_lower = [value - lower for value, lower in zip(values, lower_bounds, strict=True)]
        error_upper = [upper - value for value, upper in zip(values, upper_bounds, strict=True)]
        error_bars = [error_lower, error_upper]
    else:
        error_bars = None
    # Set the positions of the bars
    positions = range(len(values))
    # Default color
    if colors is None:
        colors = ["skyblue"] * len(values)

    # Create the bar plot
    plt.figure(figsize=(10, 5))  # Set the figure size
    bars = plt.bar(positions, values, yerr=error_bars, capsize=5, color=colors, alpha=0.75, label="Values")

    if showval:
        for bar in bars:
            yval = bar.get_height()  # Get the height of each bar
            plt.text(bar.get_x() + bar.get_width() / 2, yval, f"{yval:.3g}", ha="center", va="bottom")

    plt.xticks(positions, names)  # Set the names on the x-axis
    if ytitle:
        plt.ylabel(ytitle)

    if title:
        plt.title(title)
    # plt.legend()

    # Show the plot
    plt.grid(visible=True, linestyle="--", alpha=0.6)
    plt.show()


def plot_bars(  # noqa: C901, PLR0912 # TODO(@Anyone): refactor into something more comprehensive
    df_values: pd.DataFrame,
    color_dict: dict,
    fig_path: Path | None = None,
    ytitle: str | None = None,
    title: str | None = None,
    *,
    df_single: bool = False,
    df_shades: bool = False,
    showval: bool = True,
    single_df_shades: bool = False,
) -> None:
    """Creates a grouped bar plot with shaded regions within each bar based on another DataFrame.

    Parameters
    ----------
    df_values:
        pandas DataFrame where columns are group labels and index are the names for each group of bars (x labels)
    color_dict:
        dictionary where keys are group_labels (column names) and values are colors for the main bars
    fig_path:
        ...
    ytitle:
        string for the y-axis title
    title:
        string for the plot title
    df_single:
        ...
    df_shades:
        if plot, pandas DataFrame with the same structure as df_values,
        representing the proportion of the shaded regions within each bar
    showval:
        boolean to show the value on top of the bars
    single_df_shades:
        ...

    """
    plt.figure(figsize=(10, 5))
    names = df_values.index
    group_labels = df_values.columns
    num_groups = len(names)
    num_bars = len(group_labels)
    bar_width = 0.8 / num_bars  # Width of each bar within a group

    # Plot single df if there is any
    if isinstance(df_single, pd.DataFrame):
        single_values = df_single.to_numpy().flatten()
        single_names = df_single.index
        # get a list of single colors
        single_colors = [color_dict.get(name, "grey") for name in single_names]
        # Positions for single bars
        single_positions = np.linspace(0, -0.8, len(single_values))
        single_bars = plt.bar(single_positions, single_values, width=bar_width, color=single_colors, alpha=0.75)

        if showval:
            for bar, _, pos in zip(single_bars, single_values, single_positions, strict=True):
                yval = bar.get_height()
                plt.text(pos + bar_width / 2, yval, f"{yval:.2g}", ha="center", va="bottom")

        # plot the sahdes inside each bar
        if isinstance(single_df_shades, pd.DataFrame):
            single_shades = single_df_shades.to_numpy().flatten()
            plt.bar(single_positions, single_shades, width=bar_width, color="grey", alpha=0.5)

    # Loop through each bar in the group
    grouped_values = df_values.to_numpy()
    indices = np.arange(num_groups)  # The x locations for the groups

    for i in range(num_bars):
        values = grouped_values[:, i]
        color = color_dict.get(group_labels[i], "grey")  # Default to grey if label not found
        # Bar positions for this group
        positions = indices + (i + 2) * bar_width
        bars = plt.bar(positions, values, width=bar_width, color=color, alpha=0.75, label=group_labels[i])

        # Annotate main bar values if showval is True
        if showval:
            for bar, position, value in zip(bars, positions, values, strict=True):
                yval = bar.get_height()  # Get the height of each bar
                plt.text(position, yval, f"{value:.2g}", ha="center", va="bottom")

        # Plot shaded regions and annotate with proportions
        if isinstance(df_shades, pd.DataFrame):  # only plot the result if there is df input
            grouped_shades = df_shades.to_numpy()
            shades = grouped_shades[:, i]

            for pos, _, shade in zip(positions, values, shades, strict=True):
                plt.bar(pos, shade, width=bar_width, color="grey", alpha=0.5)
                # Annotate with the proportion value below the shaded region
                # plt.text(pos, 0, f"{shade:.3g}", ha='center', va='bottom', color='black')

    # assign all the x-axis labels
    if isinstance(df_single, pd.DataFrame):
        all_pos = np.concatenate((single_positions, indices + (num_bars + 3) * bar_width / 2), axis=None)
        all_names = list(single_names) + (list(names))
    else:
        all_pos = indices + (num_bars + 3) * bar_width / 2
        all_names = names
    plt.xticks(all_pos, list(all_names), fontsize=10)
    if ytitle:
        plt.ylabel(ytitle, fontsize=15)
    if title:
        plt.title(title)
    plt.legend()
    # Show the plot
    plt.grid(visible=True, linestyle="--", alpha=0.6)
    # save the figure to the fig_path
    if fig_path:
        plt.savefig(fig_path, dpi=300, bbox_inches="tight")
    plt.show()


def plot_miss_oov_rates(
    ref_count: nlp_tools.TokenCount,
    gen_count_list: list[nlp_tools.TokenCount],
    groupbin: int = 50,
) -> None:
    """Plots three curves regarding missing words and oovs."""
    pmiss = {}
    poov = {}
    dfreqscore = {}
    pnonword = {}
    for gen_count in gen_count_list:  # TODO(@Jing): change the format into the desired one
        msc, osc, nsc = tc_compute_miss_oov_rates(ref_count, gen_count, groupbin=groupbin)
        if gen_count.name is not None:
            pmiss[gen_count.name] = msc[["medcount", "pmiss"]]
            dfreqscore[gen_count.name] = msc[["medcount", "dfreq_score"]]
            poov[gen_count.name] = osc[["medcount", "poov"]]
            pnonword[gen_count.name] = nsc[["medcount", "pnword"]]
        else:
            print(gen_count.name)

    line_plot(pmiss)
    line_plot(dfreqscore, ylim=(-1, 1))
    line_plot(poov)
    line_plot(pnonword)


#### token count plots
def tc_plot(tokcount: nlp_tools.TokenCount) -> None:
    """Three diagnostic plots from a token count.

    - Cumulative types as a function of token counts (starting with hapaxes)
    - Cumulative tokens as a function of token counts (starting with the highest frequency word)
    - Zipf law plot
    """
    # cumulative freq plot
    sorted_data = np.sort(tokcount.df["count"])
    nbpoints = sorted_data.shape[0]

    # Compute ranks
    fraction_types = np.arange(1, len(sorted_data) + 1) / len(sorted_data)
    # Plotting
    plt.figure(figsize=(8, 5))
    plt.plot(sorted_data, fraction_types, marker="o")
    plt.title("Fraction of Types with less or equal a Token Count")
    plt.xlabel("Token Counts")
    plt.ylabel("Fraction of Total Types")
    plt.grid(True)
    plt.xscale("log")
    plt.show()

    # Compute cumulative fractions
    cumulative_counts = np.cumsum(sorted_data[::-1])[::-1]
    total_counts = cumulative_counts[0]
    fractions = cumulative_counts / total_counts

    plt.figure(figsize=(8, 5))
    plt.plot(sorted_data, fractions, marker="o")
    plt.title("Fraction of Total Tokens for Types with more of equal Token Count")
    plt.xlabel("Token Counts")
    plt.ylabel("Fraction of Total Tokens")
    plt.grid(visible=True)
    plt.xscale("log")
    plt.show()

    # Zipf law plot
    x = np.arange(1, (nbpoints + 1))  # ranks
    y = sorted_data[::-1]  # counts
    # log_x = np.log(x)
    # log_y = np.log(y)
    # Fit a linear regression model in the log-log space
    # weights = 1 / x
    # wls_model = sm.WLS(log_y, sm.add_constant(log_x), weights=weights)
    # results = wls_model.fit()
    # intercept = results.params[0]
    # slope = results.params[1]
    # log_y_fit=results.fittedvalues
    log_x, log_y_fit, intercept, slope = tokcount.zipf_coef()
    # plt.figure(figsize=(8, 5))
    plt.plot(x, y, marker="o")
    plt.plot(
        np.exp(log_x), np.exp(log_y_fit), "r-", label=f"Regression Line: y = {slope:.2f}x + {intercept:.2f}"
    )  # Regression line
    plt.title("Zipf plot")
    plt.xlabel("Rank")
    plt.ylabel("Count")
    plt.grid(visible=True)
    plt.xscale("log")
    plt.yscale("log")
    plt.legend()
    plt.show()


#################################################################################################
# Exploratory plotting functions
#################################################################################################
def tc_stats_plot(tc_stats: pd.DataFrame, keyword: str) -> None:
    """Plotting as barplot particular statistics from tc_stats.

    tc stats is a simple dataframe with columns (keywords) as type of statistics
    and lines as corpora(token counts))
    """
    bar_plot(tc_stats[keyword].to_numpy(), list(tc_stats.index), ytitle=keyword)


def tc_plot_miss_oov_rates(
    ref_count: nlp_tools.TokenCount,
    gen_count_list: list[nlp_tools.TokenCount],
    fig_path: Path,
    groupbin: int = 50,
) -> None:
    """Plots three curves regarding missing words and oovs."""
    pmiss = {}
    poov = {}
    dfreqscore = {}
    pnonword = {}

    for gen_count in gen_count_list:
        msc, osc, nsc = tc_compute_miss_oov_rates(ref_count, gen_count, groupbin=groupbin)
        if not gen_count.name:
            continue

        pmiss[gen_count.name] = msc[["medcount", "pmiss"]]
        dfreqscore[gen_count.name] = msc[["medcount", "dfreq_score"]]
        poov[gen_count.name] = osc[["medcount", "poov"]]
        pnonword[gen_count.name] = nsc[["medcount", "pnword"]]

    # save figures respectively
    line_plot(pmiss, fig_path=fig_path / "M.png")
    line_plot(dfreqscore, fig_path=fig_path / "F.png", ylim=(-1, 1))
    line_plot(poov, fig_path=fig_path / "O.png")
    line_plot(pnonword, fig_path=fig_path / "N.png")


def get_overlapped_words(str1: str, str2: str) -> str:
    """Split the strings into sets of words."""
    words_set1 = set(str1.split())
    words_set2 = set(str2.split())
    # Find the intersection of the two sets
    overlap = words_set1 & words_set2
    if len(overlap) > 0:
        return " ".join(overlap)
    return "nonwords"
